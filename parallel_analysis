import numpy as np
import pandas as pd
from sympy import symbols, lambdify
from scipy.spatial.distance import jensenshannon
from make_sense_of_RACIPE import parameter_set, steady_states
import matplotlib.pyplot as plt
from multiprocessing import Pool, cpu_count
import network_generator

# --- Helper Functions for Rate Generation ---
# (As per your instruction, this section is left as you provided it)

def _generate_production_lambdas(adjacency_matrix, gene_symbols, params_row):
    """Generates a list of numerical functions for the production rate of each gene."""
    production_odes = []
    for i in range(len(gene_symbols)):
        regulated_gene = gene_symbols[i]
        production_term = float(params_row.get('g', 50.0))
        regulation_product = 1
        for j in range(len(gene_symbols)):
            regulation_type = adjacency_matrix[i][j]
            if regulation_type != 0:
                regulator_gene = gene_symbols[j]
                param_prefix = f'{regulator_gene.name}to{regulated_gene.name}'
                s = float(params_row.get(f'thrs_{param_prefix}', 16.0))
                n = float(params_row.get(f'n', 4.0))
                if regulation_type > 0:
                    l = float(params_row.get(f'lambda_pos', 10.0))
                    K_n_pos = s**n
                    x_n_pos = regulator_gene**n
                    numerator = l + (1.0 - l) * (K_n_pos / (x_n_pos + K_n_pos))
                    regulation_product *= numerator / l
                elif regulation_type < 0:
                    l = float(params_row.get(f'lambda_neg', 0.1))
                    K_n_neg = s**n
                    x_n_neg = regulator_gene**n
                    reg_strength = K_n_neg / (x_n_neg + K_n_neg)
                    regulation_product *= l + (1.0 - l) * reg_strength
        production_odes.append(production_term * regulation_product)
    production_lambdas = [lambdify(tuple(gene_symbols), ode, 'numpy') for ode in production_odes]
    return production_lambdas

def _generate_degradation_lambdas(gene_symbols, params_row):
    """Generates a list of numerical functions for the degradation rate of each gene."""
    degradation_odes = []
    for i in range(len(gene_symbols)):
        regulated_gene = gene_symbols[i]
        k_deg = float(params_row.get(f'k', 1.0))
        degradation_odes.append(k_deg * regulated_gene)
    degradation_lambdas = [lambdify(tuple(gene_symbols), ode, 'numpy') for ode in degradation_odes]
    return degradation_lambdas

# --- Langevin Simulator ---

def simulate_chemical_langevin(production_funcs, degradation_funcs, initial_state, total_time, dt):
    """Simulates a single trajectory using the Chemical Langevin Equation (CLE)."""
    num_dimensions = len(initial_state)
    n_steps = int(total_time / dt)
    time_points = np.linspace(0, total_time, n_steps + 1)
    trajectory = np.zeros((n_steps + 1, num_dimensions))
    trajectory[0] = initial_state
    current_state = initial_state.copy()
    sqrt_dt = np.sqrt(dt)
    for i in range(n_steps):
        np.clip(current_state, 0, None, out=current_state)
        prod_rates = np.array([p_func(*current_state) for p_func in production_funcs])
        deg_rates = np.array([d_func(*current_state) for d_func in degradation_funcs])
        drift = (prod_rates - deg_rates) * dt
        
        # Generate two independent sets of random numbers for the two processes
        prod_rand = np.random.normal(size=num_dimensions)
        deg_rand = np.random.normal(size=num_dimensions)
        
        # Calculate the noise contribution from each process separately
        prod_noise = np.sqrt(prod_rates) * prod_rand * sqrt_dt
        deg_noise = np.sqrt(deg_rates) * deg_rand * sqrt_dt
        
        # --- CRITICAL FIX: ---
        # The state update must use the two separate, independent noise terms.
        current_state += drift + prod_noise - deg_noise
        
        trajectory[i + 1] = current_state
    return time_points, trajectory


# --- Modular Analysis Function ---

def analyze_trajectory_divergence(
    param_set: pd.Series,
    ss_data: pd.DataFrame,
    node_list: list,
    adjacency_matrix: np.ndarray,
    sim_time: float = 30000.0,
    dt: float = 0.01,
    num_bins: int = 30
) -> float:
    """
    Calculates the Jensen-Shannon Divergence between trajectories from the first two steady states.
    """
    if len(ss_data) < 2:
        print(f"  Warning: Fewer than 2 steady states found. Cannot calculate JSD.")
        return np.nan

    syms = symbols(node_list)
    prod_functions = _generate_production_lambdas(adjacency_matrix, syms, param_set)
    deg_functions = _generate_degradation_lambdas(syms, param_set)

    traj_arrays = []
    for i in range(len(ss_data)):
        temp_ss = ss_data.iloc[i]
        start_state = np.array([2**float(temp_ss[node]) for node in node_list], dtype=np.float64)
        _, traj = simulate_chemical_langevin(prod_functions, deg_functions, start_state, sim_time, dt)
        traj_arrays.append(traj)

    all_points = np.vstack(traj_arrays)
    bin_min = np.min(all_points, axis=0)
    bin_max = np.max(all_points, axis=0)
    
    bins = [np.linspace(bin_min[i], bin_max[i], num_bins) for i in range(len(node_list))]

    p_counts, _ = np.histogramdd(traj_arrays[0], bins=bins)
    q_counts, _ = np.histogramdd(traj_arrays[1], bins=bins)

    epsilon = 1e-10
    P = p_counts.ravel() / (p_counts.sum() + epsilon)
    Q = q_counts.ravel() / (q_counts.sum() + epsilon)

    js_divergence_sq = jensenshannon(P, Q, base=2)**2
    return js_divergence_sq

# --- Worker function for multiprocessing ---
def run_analysis_for_param(task_dict):
    """
    Worker function that unpacks a dictionary and runs the main analysis.
    This is what each parallel process will execute.
    """
    param_id = task_dict['param_id']
    print(f"  Starting analysis for Parameter Set ID: {param_id}")
    
    jsd = analyze_trajectory_divergence(
        param_set=task_dict['param_set'],
        ss_data=task_dict['ss_data'],
        node_list=task_dict['node_list'],
        adjacency_matrix=task_dict['adjacency_matrix']
    )
    
    return {'param_id': param_id, 'realisation': task_dict['realisation'], 'JSD': jsd}

# --- Commented-out Test Block (Corrected) ---
"""
if __name__ == '__main__':
    adj_matrix=[[1,-1],[-1,1]]
    nodes=symbols('A,B')
    dummy_params=parameter_set("MISA")[parameter_set('MISA')['PS.No']==50].iloc[0]
    init_states=np.array([10,100])
    
    # CORRECTED: _generate_degradation_lambdas does not use adj_matrix
    prod_funcs=_generate_production_lambdas(adj_matrix,nodes,dummy_params)
    deg_funcs=_generate_degradation_lambdas(nodes,dummy_params)

    time_array,traj=simulate_chemical_langevin(prod_funcs,deg_funcs,init_states,10000,0.01)

    # CORRECTED: plotting call should be time_array vs traj
    plt.plot(time_array, traj[:,0], label='A')
    plt.plot(time_array, traj[:,1], label='B')
    plt.legend()
    plt.show()
"""
# --- Main Execution Block for Parallel Processing ---
if __name__ == '__main__':
    # 1. Define network properties
    network_name = "0_four_node"
    adj_matrix, nodes = network_generator.topo_to_adj(f"{network_name}.topo")
    NUM_REALISATIONS = 10

    # 2. Load all parameters and steady states once
    all_params = pd.read_csv(f"{network_name}_parameters.csv")
    all_ss = pd.read_csv(f"{network_name}_solution.csv")

    # 3. Create a list of all tasks to be executed
    tasks = []
    # Using the first 5 unique parameter IDs as an example
    for param_id in all_params['para_indx'].unique()[2:3]:
        current_params = all_params[all_params['para_indx'] == param_id].iloc[0]
        current_ss = all_ss[all_ss['para_indx'] == param_id]

        # Add a task for each realisation of this parameter set
        for i in range(NUM_REALISATIONS):
            task = {
                'param_id': param_id,
                'realisation': i + 1,
                'param_set': current_params,
                'ss_data': current_ss,
                'node_list': nodes,
                'adjacency_matrix': adj_matrix
            }
            tasks.append(task)
            
    # 4. Use a multiprocessing Pool to execute tasks in parallel
    num_cores = cpu_count()
    print(f"\n--- Starting parallel analysis on {num_cores} cores ---")
    
    with Pool(processes=num_cores) as pool:
        all_results_list = pool.map(run_analysis_for_param, tasks)

    # 5. After all parallel tasks are done, create and save the final DataFrame
    if all_results_list:
        results_df = pd.DataFrame(all_results_list)
        results_df.sort_values(by=['param_id', 'realisation'], inplace=True)

        print("\n--- Final JSD Results (All Runs) ---")
        print(results_df.to_string())

        summary_df = results_df.groupby('param_id')['JSD'].agg(['mean', 'std', 'count']).reset_index()
        print("\n--- Summary Statistics per Parameter ID ---")
        print(summary_df)

        output_filename = "jsd_parallel_analysis.csv"
        results_df.to_csv(output_filename, index=False)
        print(f"\nDetailed results saved to {output_filename}")
    else:
        print("\nNo results were generated.")